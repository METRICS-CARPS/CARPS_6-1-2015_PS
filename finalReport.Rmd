---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r}
articleID <- "6-1-2015_PS" # insert the article ID code here e.g., "10-3-2015_PS"
reportType <- 'final'
pilotNames <- "Elizabeth Blevins, Barbara Barbosa Born" # insert the pilot's name here e.g., "Tom Hardwicke". If there are multiple pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- "Manuel Bohn" # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- 420 # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- 60 # insert the co- pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- as.Date("10/27/2017", format = "%m/%d/%y") # insert the pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- as.Date("10/19/2018", format = "%m/%d/%y") # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- as.Date("10/19/2018", format = "%m/%d/%y") # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

-------

#### Methods summary: 

The target outcomes are from Experiment 1, in which ten participants took part. Participants completed two blocks consisting each of a different task. In the "grasping task", participants lifted a target disk from the table using their thumb and forefinger; in the "manual estimation task", participants used their thumb and forefinger to indicate the size of the target before physically lifting it. Within each block, the target disk varied in size (3.00 cm or 3.75 cm) and whether it was presented on the table by itself or accompanied by other disks. Additionally, participants wore liquid-crystal goggles. On half of the trials, which the researchers refer to as "open-loop" trials, the goggles were closed at the beginning of the trial meaning "that participants could not see their moving hand or the disks" (Chen, Sperandio, & Goodale, 2015, p. 60). On the remaining trials, which were labeled as "closed-loop", the goggles were closed after three seconds, allowing participants to track both their motion and the target.

Experiment 1 consisted of a 2 Task (grip, estimate) X 2 View (closed-loop, open-loop) X 2 Condition (isolated, crowded) X 2 Size (3.00 cm, 3.75 cm) within subjects design. Participants completed each of the 16 trial types ten times, resulting in a total of 160 trials. For each trial type, the researchers averaged participants' grip and size estimates. "A four-way repeated measures analysis of variance (ANOVA) was used to analyze the main effects of crowding condition, viewing condition, task, and target size, as well as their interactions" (Chen, Sperandio, & Goodale, 2015, p. 62). To follow up results from the ANOVA, the researchers then conducted paired *t*-tests, which they specified as two-tailed tests.

------

#### Target outcomes: 

>"Experiment 1 was designed to explore the effects of crowding on perception and action, with a particular focus on whether participants could scale their grip aperture to the size of the target even when they could not consciously identify the size of the target. We carried out a four-way repeated measures ANOVA on the manual estimates and PGAs with task (estimation vs. grasping), crowding condition (uncrowded vs. crowded), viewing condition (closed- vs. open-loop), and target size (3.0 vs. 3.75 cm) as main factors. The significant interaction between task and crowding condition, F(1, 9) = 6.818, p = .028, suggested that crowding had different effects on performance of the grasping and manual estimation tasks. Not surprisingly, when the target was presented in isolation, participants were able to manually estimate the sizes of the two targets—and this was true for both closed-loop trials, t(9) = 7.23, p < .001, and open-loop trials, t(9) = 9.19, p < .001. Similarly, participants showed excellent grip scaling for targets presented in isolation on both closed-loop trials, t(9) = 4.29, p = .002, and openloop trials, t(9) = 4.79, p = .001 (Fig. 3). Things were quite different, however, when the target disks were surrounded by flankers. In this condition, par- ticipants could no longer discriminate between the two disk sizes using a manual estimate—closed-loop trials: t(9) = 1.02, p = .334; open-loop trials: t(9) = 1.78, p = .108 presumably because the size of the target was per- ceptually invisible. (Note that we use the term invisible to refer to the fact that participants could not identify the size of the target, even though they were aware of its presence and position.) In contrast, when participants were asked to grasp the same targets, their PGAs were still scaled to target size—closed-loop trials: t(9) = 4.21, p = .002; open-loop trials: t(9) = 3.392, p = .008 (Fig. 3)." (Chen, Sperandio, & Goodale, 2015, pp 62-63).

------

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Step 1: Load packages

```{r}
# load packages
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
library(ez) # for repeated measures ANOVA's
library(broom)
```

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared.
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

## Step 2: Load data

I read in the data file, "data_Exp1.xlsx", from the "summary" tab.

```{r}
# read data from Exp 1 summary sheet
# ignore column names
d <- read_xlsx("data/data_Exp1.xlsx", sheet ="summary", range = cell_rows(4:13), col_names = FALSE)

```

## Step 3: Tidy data

```{r}
# organize data in wide format
d_wide <- d %>%
  select(-c(X__6, X__11, X__16)) %>% # drop empty columns
  # rename columns according to condition
  rename(subj = X__1,
         grip_clos_isol_3.00cm = X__2,
         grip_clos_isol_3.75cm = X__3,
         grip_clos_crow_3.00cm = X__4,
         grip_clos_crow_3.75cm = X__5,
         grip_open_isol_3.00cm = X__7,
         grip_open_isol_3.75cm = X__8,
         grip_open_crow_3.00cm = X__9,
         grip_open_crow_3.75cm = X__10,
         esti_clos_isol_3.00cm = X__12,
         esti_clos_isol_3.75cm = X__13,
         esti_clos_crow_3.00cm = X__14,
         esti_clos_crow_3.75cm = X__15,
         esti_open_isol_3.00cm = X__17,
         esti_open_isol_3.75cm = X__18,
         esti_open_crow_3.00cm = X__19,
         esti_open_crow_3.75cm = X__20)

# check structure
str(d_wide)

# convert from wide to long format
d_tidy <- d_wide %>%
  gather(trial_type, size_measurement_cm, grip_clos_isol_3.00cm:esti_open_crow_3.75cm) %>%
  # separate trial_type into four columns
  separate(trial_type, into = c("task","view","cond","size"), sep = "\\_") %>%
  # convert trial types to factors
  mutate(task = factor(task, levels = c("grip", "esti")),
         view = factor(view, levels = c("clos", "open")),
         cond = factor(cond, levels = c("isol", "crow")),
         size = factor(size, levels = c("3.75cm", "3.00cm")))

str(d_tidy)

# task: grasping (grip) vs. manual (esti)
# view: open (open) vs. closed (clos)
# cond: isolated (isol) vs. crowded (crow)
# size: 3.00 vs. 3.75 cm
```

## Step 4: Run analysis

### Descriptive statistics
I began the analysis by computing the mean and standard deviation within each of the sixteen conditions. While not explicitly part of the target outcomes, I wanted to compare the values with those depicted in Figure 3.
```{r}
# compute mean and standard deviation per condition
descriptives = d_tidy %>%
  group_by(task, view, cond, size) %>%
  summarise(mean = mean(size_measurement_cm),
            stdv = sd(size_measurement_cm))

descriptives_table <- kable(descriptives, digits = 2) # round to two digits
descriptives_table
```

### Inferential statistics
Next, I conducted a four-way repeated measures ANOVA whereby `task`, `view`, `condition`, and `size` were treated as within subjects factors. Additionally, I specified Type III sum of squares in the model.

From the target outcomes: 
> The significant interaction between task and crowding condition, F(1, 9) = 6.818, p = .028, suggested that crowding had different effects on performance of the grasping and manual estimation tasks.

```{r}
# conduct repeated measures ANOVA
# specified type III SS
model <- ezANOVA(d_tidy, dv = .(size_measurement_cm), wid = .(subj), within = .(task, view, cond, size), detailed = TRUE, type = "III")
# return model
print(model) %>% kable(digits = 3)
```

I then conducted paired *t*-tests to decompose the interaction. (The target outcomes require only the first four *t*-tests, although I ran all pairwise comparisons.)

From the target outcomes: 
> Not surprisingly, when the target was presented in isolation, participants were able to manually estimate the sizes of the two targets and this was true for both closed-loop trials, t(9) = 7.23, p < .001, and open-loop trials, t(9) = 9.19, p < .001. Similarly, participants showed excellent grip scaling for targets presented in isolation on both closed-loop trials, t(9) = 4.29, p = .002, and openloop trials, t(9) = 4.79, p = .001 (Fig. 3) Things were quite different, however, when the target disks were surrounded by flankers. In this condition, par- ticipants could no longer discriminate between the two disk sizes using a manual estimate—closed-loop trials: t(9) = 1.02, p = .334; open-loop trials: t(9) = 1.78, p = .108 presumably because the size of the target was per- ceptually invisible. (Note that we use the term invisible to refer to the fact that participants could not identify the size of the target, even though they were aware of its presence and position.) In contrast, when participants were asked to grasp the same targets, their PGAs were still scaled to target size—closed-loop trials: t(9) = 4.21, p = .002; open-loop trials: t(9) = 3.392, p = .008 (Fig. 3)." (Chen, Sperandio, & Goodale, 2015, pp 62-63).


```{r}
# paired t-tests to compare 3cm vs. 3.75cm
# manual estimation of isolated targets on closed-loop trials
d_tidy_filter <- d_tidy %>%
  filter(task == "esti",
         cond == "isol",
         view == "clos")
ttest_esti_isol_clos = t.test(size_measurement_cm ~ size, data = d_tidy_filter, paired = TRUE)

# manual estimation of isolated targets on open-loop trials
d_tidy_filter <- d_tidy %>%
  filter(task == "esti",
         cond == "isol",
         view == "open")
ttest_esti_isol_open = t.test(size_measurement_cm ~ size, data = d_tidy_filter, paired = TRUE)

# grip of isolated targets on closed-loop trials
d_tidy_filter <- d_tidy %>%
  filter(task == "grip",
         cond == "isol",
         view == "clos")
ttest_grip_isol_clos = t.test(size_measurement_cm ~ size, data = d_tidy_filter, paired = TRUE)

# grip of isolated targets on open-loop trials
d_tidy_filter <- d_tidy %>%
  filter(task == "grip",
         cond == "isol",
         view == "open")
ttest_grip_isol_open = t.test(size_measurement_cm ~ size, data = d_tidy_filter, paired = TRUE)

# manual estimation of crowded targets on closed-loop trials
d_tidy_filter <- d_tidy %>%
  filter(task == "esti",
         cond == "crow",
         view == "clos")
ttest_esti_crow_clos = t.test(size_measurement_cm ~ size, data = d_tidy_filter, paired = TRUE)

# manual estimation of crowded targets on open-loop trials
d_tidy_filter <- d_tidy %>%
  filter(task == "esti",
         cond == "crow",
         view == "open")
ttest_esti_crow_open = t.test(size_measurement_cm ~ size, data = d_tidy_filter, paired = TRUE)

# grip of crowded targets on closed-loop trials
d_tidy_filter <- d_tidy %>%
  filter(task == "grip",
         cond == "crow",
         view == "clos")
ttest_grip_crow_clos = t.test(size_measurement_cm ~ size, data = d_tidy_filter, paired = TRUE)

# grip of crowded targets on open-loop trials
d_tidy_filter <- d_tidy %>%
  filter(task == "grip",
         cond == "crow",
         view == "open")
ttest_grip_crow_open = t.test(size_measurement_cm ~ size, data = d_tidy_filter, paired = TRUE)

# combine t-test results in one table
ttest_results <- map_df(list(ttest_esti_isol_clos, ttest_esti_isol_open, ttest_grip_isol_clos, ttest_grip_isol_open,
                    ttest_esti_crow_clos, ttest_esti_crow_open, ttest_grip_crow_clos, ttest_grip_crow_open), tidy)

# add column names
ttest_type = c("esti_isol_clos","esti_isol_open","grip_isol_clos","grip_isol_open","esti_crow_clos","esti_crow_open","grip_crow_clos","grip_crow_open")
ttest_results_name <- cbind(ttest_type,ttest_results)

# only include columns related to stats in manuscript
ttest_results_clean <- kable(ttest_results_name[c("ttest_type","estimate", "statistic", "p.value", "parameter", "method", "alternative")])
ttest_results_clean
```

Lastly, I used the `compareValues` function to record the number of errors. For two *t*-tests, the associated *p*-values were recorded as *p* < .001 in the manuscript. I could not use the `compareValues` function, but since the *p*-values I obtained similarly fell below .001, I did not count them as errors. (I consulted with Tom via email regarding this issue.)
```{r echo=FALSE}
# save statistics for conclusion
# anova
anova_taskxcond_DFn <- model$ANOVA$DFn[[7]]
anova_taskxcond_DFd <- model$ANOVA$DFd[[7]]
anova_taskxcond_fval <- model$ANOVA$F[[7]]
anova_taskxcond_pval <- model$ANOVA$p[[7]]

# ttest for manual estimation of isolated targets on closed-loop trials (compare 3cm vs. 3.75cm)
ttest1_tval <- ttest_results$statistic[[1]]
ttest1_df <- ttest_results$parameter[[1]]
ttest1_pval <- ttest_results$p.value[[1]] # with rounding p = 0 so include p < .001

# ttest for manual estimation of isolated targets on open-loop trials (compare 3cm vs. 3.75cm)
ttest2_tval <- ttest_results$statistic[[2]]
ttest2_df <- ttest_results$parameter[[2]]
ttest2_pval <- ttest_results$p.value[[2]] # with rounding p = 0 so include p < .001

# ttest for grip of isolated targets on closed-loop trials (compare 3cm vs. 3.75cm)
ttest3_tval <- ttest_results$statistic[[3]]
ttest3_df <- ttest_results$parameter[[3]]
ttest3_pval <- ttest_results$p.value[[3]]

# ttest for grip of isolated targets on open-loop trials (compare 3cm vs. 3.75cm)
ttest4_tval <- ttest_results$statistic[[4]]
ttest4_df <- ttest_results$parameter[[4]]
ttest4_pval <- ttest_results$p.value[[4]]

# ttest for manual estimation of crowded targets on closed-loop trials (compare 3cm vs. 3.75cm)
ttest5_tval <- ttest_results$statistic[[5]]
ttest5_df <- ttest_results$parameter[[5]]
ttest5_pval <- ttest_results$p.value[[5]]

# ttest for manual estimation of crowded targets on open-loop trials (compare 3cm vs. 3.75cm)
ttest6_tval <- ttest_results$statistic[[6]]
ttest6_df <- ttest_results$parameter[[6]]
ttest6_pval <- ttest_results$p.value[[6]]

# ttest for grip of crowded targets on closed-loop trials (compare 3cm vs. 3.75cm)
ttest7_tval <- ttest_results$statistic[[7]]
ttest7_df <- ttest_results$parameter[[7]]
ttest7_pval <- ttest_results$p.value[[7]]

# ttest for grip of crowded targets on open-loop trials (compare 3cm vs. 3.75cm)
ttest8_tval <- ttest_results$statistic[[8]]
ttest8_df <- ttest_results$parameter[[8]]
ttest8_pval <- ttest_results$p.value[[8]]
```

```{r}
# compare ANOVA results
# df numerator
reportObject <- reproCheck(reportedValue = "1", obtainedValue = anova_taskxcond_DFn, valueType = 'df')
# df denominator
reportObject <- reproCheck(reportedValue = "9", obtainedValue = anova_taskxcond_DFd, valueType = 'df')
# F statistic
reportObject <- reproCheck(reportedValue = "6.818", obtainedValue = anova_taskxcond_fval, valueType = 'F')
# p value
reportObject <- reproCheck(reportedValue = ".028", obtainedValue = anova_taskxcond_pval, valueType = 'p')

# compare t-test results
# ttest for manual estimation of isolated targets on closed-loop trials (compare 3cm vs. 3.75cm)
# t df
reportObject <- reproCheck(reportedValue = "9", obtainedValue = ttest1_df, valueType = 'df')
# t statistic
reportObject <- reproCheck(reportedValue = "7.23", obtainedValue = ttest1_tval, valueType = 't')
# p value reported as < .001

# ttest for manual estimation of isolated targets on open-loop trials (compare 3cm vs. 3.75cm)
# t df
reportObject <- reproCheck(reportedValue = "9", obtainedValue = ttest2_df, valueType = 'df')
# t statistic
reportObject <- reproCheck(reportedValue = "9.19", obtainedValue = ttest2_tval, valueType = 't')
# p value reported as < .001

# ttest for grip of isolated targets on closed-loop trials (compare 3cm vs. 3.75cm)
# t df
reportObject <- reproCheck(reportedValue = "9", obtainedValue = ttest3_df, valueType = 'df')
# t statistic
reportObject <- reproCheck(reportedValue = "4.29", obtainedValue = ttest3_tval, valueType = 't')
# t p value
reportObject <- reproCheck(reportedValue = ".002", obtainedValue = ttest3_pval, valueType = 'p')

# ttest for grip of isolated targets on open-loop trials (compare 3cm vs. 3.75cm)
# t df
reportObject <- reproCheck(reportedValue = "9", obtainedValue = ttest4_df, valueType = 'df')
# t statistic
reportObject <- reproCheck(reportedValue = "4.79", obtainedValue = ttest4_tval, valueType = 't')
# t p value
reportObject <- reproCheck(reportedValue = ".001", obtainedValue = ttest4_pval, valueType = 'p')

# compare t-test results
# ttest for manual estimation of crowded targets on closed-loop trials (compare 3cm vs. 3.75cm)
# t df
reportObject <- reproCheck(reportedValue = "9", obtainedValue = ttest5_df, valueType = 'df')
# t statistic
reportObject <- reproCheck(reportedValue = "1.02", obtainedValue = ttest5_tval, valueType = 't')
# t p value
reportObject <- reproCheck(reportedValue = ".334", obtainedValue = ttest5_pval, valueType = 'p')

# compare t-test results
# ttest for manual estimation of crowded targets on open-loop trials (compare 3cm vs. 3.75cm)
# t df
reportObject <- reproCheck(reportedValue = "9", obtainedValue = ttest6_df, valueType = 'df')
# t statistic
reportObject <- reproCheck(reportedValue = "1.78", obtainedValue = ttest6_tval, valueType = 't')
# t p value
reportObject <- reproCheck(reportedValue = ".108", obtainedValue = ttest6_pval, valueType = 'p')

# compare t-test results
# ttest for grip of crowded targets on closed-loop trials (compare 3cm vs. 3.75cm)
# t df
reportObject <- reproCheck(reportedValue = "9", obtainedValue = ttest7_df, valueType = 'df')
# t statistic
reportObject <- reproCheck(reportedValue = "4.21", obtainedValue = ttest7_tval, valueType = 't')
# t p value
reportObject <- reproCheck(reportedValue = ".002", obtainedValue = ttest7_pval, valueType = 'p')

# compare t-test results
# ttest for grip of crowded targets on open-loop trials (compare 3cm vs. 3.75cm)
# t df
reportObject <- reproCheck(reportedValue = "9", obtainedValue = ttest8_df, valueType = 'df')
# t statistic
reportObject <- reproCheck(reportedValue = "3.392", obtainedValue = ttest8_tval, valueType = 't')
# t p value
reportObject <- reproCheck(reportedValue = ".008", obtainedValue = ttest8_pval, valueType = 'p')
```

## Step 5: Conclusion

The study results were successfully replicated. It appears the researchers consistently rounded the *t*-statistics down, producing three minor numerical errors. In the following summary, I report the statistics to three decimal places.

A 2 Task (grip, estimate) X 2 View (closed-loop, open-loop) X 2 Condition (isolated, crowded) X 2 Size (3.00 cm, 3.75 cm) repeated measures ANOVA revealed a significant Task by Condition interaction, *F*(`r anova_taskxcond_DFn`, `r anova_taskxcond_DFd`) = `r round(anova_taskxcond_fval, digits=3)`, *p* = `r round(anova_taskxcond_pval, digits=3)`. I then conducted post-hoc *t*-tests to decomposed the interation.

Comparing performance in the manual estimation condition where the target disk was presented by itself, participants successfully differentiated between the two disk sizes on closed-loop trials, *t*(`r ttest1_df`) = `r round(ttest1_tval, digits=3)`, *p* < .001, as well as on open-loop trials, *t*(`r ttest2_df`) = `r round(ttest2_tval, digits=3)`, *p* < .001. A similar pattern of results was obtained when participants lifted the target disk in the isolation condition. Again, participants distinguished between the two disk sizes on closed-loop trials, *t*(`r ttest3_df`) = `r round(ttest3_tval, digits=3)`, *p* = `r round(ttest3_pval, digits=3)`, and on open-loop trials, *t*(`r ttest4_df`) = `r round(ttest4_tval, digits=3)`, *p* = `r round(ttest4_pval, digits=3)`.

In the crowded condition, paricipants did not distinguish between disk sizes in when manually estimating its size (closed loop trials: *t*(`r ttest5_df`) = `r round(ttest5_tval, digits=3)`, *p* = `r round(ttest5_pval, digits=3)`; open loop trials: *t*(`r ttest6_df`) = `r round(ttest6_tval, digits=3)`, *p* = `r round(ttest6_pval, digits=3)`). In contrast, they still distinguished between sizes when manually grasping the disk (closed loop trials: *t*(`r ttest7_df`) = `r round(ttest7_tval, digits=3)`, *p* = `r round(ttest7_pval, digits=3)`; open loop trials: *t*(`r ttest8_df`) = `r round(ttest8_tval, digits=3)`, *p* = `r round(ttest8_pval, digits=3)`).

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)

Insufficient_Information_Errors <- 0 # how many discrete insufficient information issues did you encounter?

# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).

locus_typo <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- NA # how many discrete issues were there for which you could not identify the cause

Affects_Conclusion <- NA # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add the articleID 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome != "MATCH") | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

# Session information

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
